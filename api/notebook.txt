/console/api/datasets/init
    1、如果索引方式是 high_quality ，则判断是否有 Embedding 模型，然后调用 save_document_without_dataset_id 方法
    2、如果索引方式是 high_quality，则需要为 dataset 创建一个 dataset_collection_binding，获取到 dataset_collection_binding_id
         如果索引方式不是 high_quality，则 dataset_collection_binding_id 为 None
    3、同理，如果索引方式是 high_quality，则需要获取到 embedding_model 和 retrieval_model
         如果索引方式不是 high_quality，则 embedding_model，retrieval_model 为 None
    4、创建 DataSet，需要用到上述的 retrieval_model，collection_binding_id，embedding_model 等参数
    5、调用 save_document_with_dataset_id 方法
    6、如果 dataset 为空，则更新dataset 的 data_source_type，indexing_technique
    7、绑定当前 dataset 的 ProcessRule，便于后续用到
    8、如果当前文件是上传的，那么循环上传的文件列表创建或更新 document
            根据文件名称，dataset id，租户名称等进行查询，如果有则更新 document，没有则新建 document
            将之前就有的 document 的 id 放入 duplicate_document_ids 中，新建的 document id 放入 document_ids 中
    9、将新建的 document 列表传入  document_indexing_task 异步任务中
    10、将更新的 document 列表传入 duplicate_document_indexing_task 异步任务中


document_indexing_task
    1、根据传入的 dataset_id 获取 dataset 对象
    2、判断一些限制
    3、根据传入的 document_ids，循环更新 document 对象，更新 indexing_status 为 parsing，更新 processing_started_at 时间
    4、初始化 IndexingRunner，执行 run 方法，传入 documents 对象列表
    5、查询 DatasetProcessRule 的 processing_rule。得到 处理规则
    6、根据 index_type 获取到 index_processor 对象。
    7、调用 self._extract 提取文本，返回值为 list[Document]
            self._extract 底层调用 index_processor.extract，最终调用 ExtractProcessor.extract
            更新 after_indexing_status 值为 splitting
    8、将提取出的 list[Document] 传入 self._transform
            self._transform 底层调用 index_processor.transform
    9、transform 中首先根据 process_rule 获取到一个 splitter
    10、循环传入的 documents 列表（步骤7提取的结果）
            在循环中为每个 document 对象添加 doc_id 和 doc_hash 的元数据。doc_id 是 uuid，doc_hash 则是根据内容计算的
            在循环中，第一步调用 CleanProcessor.clean 清除额外信息（空格，邮箱，用户可在前端选择）
            第二步调用 splitter.split_documents([document])，创建 Document 对象。这里叫做 document_nodes
            第三步为每个 Document 对象赋予一个 doc_id 和 doc_hash 值。并且去除文字中的句号
    11、返回 all_documents ，里面是所有处理好的 Document，是一个列表
    12、调用  self._load_segments(dataset, dataset_document, documents)，documents 是 11 步的返回结果
            创建一个 DatasetDocumentStore 实例
            调用 add_documents 方法，将 documents 列表中的每条数据，采用 create or update 的策略保存为一个 DocumentSegment 对象
    13、更新 document 的 after_indexing_status 值为 indexing，更新 segment 的状态为 indexing
    14、调用 self._load
            新启一个线程，用于创建关键字索引，执行的方法为 self._process_keyword_index
                    self._process_keyword_index 中先获取了dataset 对象，然后创建一个 Keyword 实例。
                            Keyword.create 使用 Jieba分词 进行关键词的提取，并且将关键词保存到 document_segment 对象中
                                    最后将关键词保存到数据库中，或者文件系统中
                    将之前标记的 indexing 状态的 DocumentSegment 的状态更新为 completed
            创建一个线程池，执行 self._process_chunk 方法
                    self._process_chunk 先预估大致会消耗的 token 的数量
                    调用 index_processor.load，底层调用 ParagraphIndexProcessor.load
                            load 先是创建 Vector 实例，然后调用 create 方法创建 embeddings
                                    第一步：调用 embed_documents，会根据每个 document.page_content  的hash值，使用的模型，provider 的名称来判断该段文件之前是否向量化过
                                            如果已经向量化过了，那么直接从数据库中查询结果即可。否则将文档下标标记加入一个 embedding_queue_indices 列表中
                                            如果没有向量化，那么获取到当前模型的 max_chunks，按照 max_chunks 拆分这个文本，得到每次向量化的 chunk 内容
                                            然后调用 self._model_instance.invoke_text_embedding 方法进行对应模型的，向量化接口的调用。会调用 invoke 方法，然后调用 self._invoke。self._invoke 对应的就是每个模型实例的向量化接口的具体实现
                                            得到向量化的结果后，会和文本的 hash 值进行绑定，最终存储在 Embedding 表中。会记录 模型名称，provider，hash值，pickle 序列化后的 embedding 值。
                                            如此往复，直到所有文本都向量化结束，最后返回所有向量列表，向量值的顺序就是文本的顺序
                                    第二步：得到向量结果后（可能是新生成的，也可能是直接从数据库获取到以前的）需要把向量保存到向量数据库中
                                            调用 self._vector_processor.create 方法，create 是一个抽象方法，下面有很多的向量数据库的具体实现
                                            这里有个疑问？假如文本相同，那么向量数据库中会存储相同的数据，不会进行去重判断？
                                            如果一段文本进行不同模型的向量化，那的确是可能的。那如何防止同一段文本，同一种模型，保存重复的结果呢?
                    将 DocumentSegment 状态从 indexing 更新为 completed
            更新 document 的 after_indexing_status 状态为 completed


duplicate_document_indexing_task
    1、根据传入的 dataset_id 获取 dataset 对象
    2、判断一些限制
    3、遍历每一个 document_id, 获取到每一个 document_id 的 DocumentSegment 对象
    4、获取所有 DocumentSegment 对象的 index_node_id，得到一个 index_node_ids 列表
    5、调用 index_processor.clean 方法，删除这些 index_node_id
            clean 方法一个接口，具体的实现中，如果索引技术是 high_quality，则需要删除向量数据库中的数据。
                    vector.delete_by_ids(node_ids) 方法中，node_ids 就是传入的 index_node_id。会去向量数据库中删除这些数据
                    在向量数据的插入方法 add_texts 中，也会从 document 对象的 metadata 中提取出 doc_id 进行插入。所以删除的时候就可以根据这些 id 进行查询删除了。
            如果需要删除 keywords，则也会将 keyword 进行删除
    6、然后再在数据库中删除这些 segment
    7、处理完旧数据后，和 document_indexing_task 一样，会创建 IndexingRunner 对象，调用 indexing_runner.run(documents) 方法，传入  documents 对象列表
    8、接下来就和 document_indexing_task 任务中的步骤5接下来的一致了。
    9、总结下来就是多了一个删除旧数据的过程（删除向量数据库中的数据和后端缓存中的数据，其中后端的删除中，并没有删除Embedding表中的数据，只是删除了 DocumentSegment 记录）

    index_node_id 是什么时候绑定到 DocumentSegment 中的，又是怎么和向量数据库进行绑定的？因为删除的时候这个ID显得尤为重要？需要根据这个 ID 去向量数据库中进行数据的删除！
        DocumentSegment 对象 index_node_id 的绑定，是在 document_indexing_task 过程中的，self._load_segments -> add_documents 方法下进行绑定的
        这里在创建 DocumentSegment 对象的时候，将 index_node_id 的值，设置为了 当前文档元数据中的doc_id，即 doc.metadata['doc_id']。


document_indexing_update_task
    1、传入参数为 dataset_id，document_id。首先获取到 document 对象。
    2、将 document indexing_status 状态标记为 parsing
    3、查询 document 对应的 segments 列表，获取到所有的 index_node_id
    4、调用  index_processor.clean(dataset, index_node_ids) 在向量数据库中删除这些数据
    5、再在数据库中删除 segments 数据
    6、最后重新实例化 IndexingRunner 对象，调用 indexing_runner.run([document]) 方法，重新将 document 向量化


对比 Langchain 的实现：
    https://python.langchain.com/v0.1/docs/modules/data_connection/indexing/
    LangChain 利用记录管理器（RecordManager）来管理索引
    在索引内容时，会为每个文档计算哈希值，并将以下信息存储在记录管理器中：
        1、文档哈希（页面内容和元数据的哈希）
        2、时间
        3、source id
    Langchain 提供了三种模式供选择，分别是 None，Incremental，Full
        None 模式：此模式不会自动清理旧版本的内容；但是，它仍会进行内容重复数据删除。举例：当我进行三份相同文本进行索引时，最终向量数据库中只会有一份。且以前的数据不影响。
        incremental 模式：对相同文本再次索引时将会被跳过。但是如果对已经存在的索引进行更新，那么则会写入新版本，并删除所有共享同一 source 的旧版本
        full 模式：任何未传递到索引函数且存在于向量存储中的文档都将被删除。也就是说只会保存当前我提交的，之前的都会被删除。









