/console/api/datasets/init
    1、如果索引方式是 high_quality ，则判断是否有 Embedding 模型，然后调用 save_document_without_dataset_id 方法
    2、如果索引方式是 high_quality，则需要为 dataset 创建一个 dataset_collection_binding，获取到 dataset_collection_binding_id
         如果索引方式不是 high_quality，则 dataset_collection_binding_id 为 None
    3、同理，如果索引方式是 high_quality，则需要获取到 embedding_model 和 retrieval_model
         如果索引方式不是 high_quality，则 embedding_model，retrieval_model 为 None
    4、创建 DataSet，需要用到上述的 retrieval_model，collection_binding_id，embedding_model 等参数
    5、调用 save_document_with_dataset_id 方法
    6、如果 dataset 为空，则更新dataset 的 data_source_type，indexing_technique
    7、绑定当前 dataset 的 ProcessRule，便于后续用到
    8、如果当前文件是上传的，那么循环上传的文件列表创建或更新 document
            根据文件名称，dataset id，租户名称等进行查询，如果有则更新 document，没有则新建 document
            将之前就有的 document 的 id 放入 duplicate_document_ids 中，新建的 document id 放入 document_ids 中
    9、将新建的 document 列表传入  document_indexing_task 异步任务中
    10、将更新的 document 列表传入 duplicate_document_indexing_task 异步任务中


document_indexing_task
    1、根据传入的 dataset_id 获取 dataset 对象
    2、判断一些限制
    3、根据传入的 document_ids，循环更新 document 对象，更新 indexing_status 为 parsing，更新 processing_started_at 时间
    4、初始化 IndexingRunner，执行 run 方法，传入 documents 对象列表
    5、查询 DatasetProcessRule 的 processing_rule。得到 处理规则
    6、根据 index_type 获取到 index_processor 对象。
    7、调用 self._extract 提取文本，返回值为 list[Document]
            self._extract 底层调用 index_processor.extract，最终调用 ExtractProcessor.extract
            更新 after_indexing_status 值为 splitting
    8、将提取出的 list[Document] 传入 self._transform
            self._transform 底层调用 index_processor.transform
    9、transform 中首先根据 process_rule 获取到一个 splitter
    10、循环传入的 documents 列表（步骤7提取的结果）
            在循环中为每个 document 对象添加 doc_id 和 doc_hash 的元数据。doc_id 是 uuid，doc_hash 则是根据内容计算的
            在循环中，第一步调用 CleanProcessor.clean 清除额外信息（空格，邮箱，用户可在前端选择）
            第二步调用 splitter.split_documents([document])，创建 Document 对象。这里叫做 document_nodes
            第三步为每个 Document 对象赋予一个 doc_id 和 doc_hash 值。并且去除文字中的句号
    11、返回 all_documents ，里面是所有处理好的 Document，是一个列表
    12、调用  self._load_segments(dataset, dataset_document, documents)，documents 是 11 步的返回结果
            创建一个 DatasetDocumentStore 实例
            调用 add_documents 方法，将 documents 列表中的每条数据，采用 create or update 的策略保存为一个 DocumentSegment 对象
    13、更新 document 的 after_indexing_status 值为 indexing，更新 segment 的状态为 indexing
    14、调用 self._load
            新启一个线程，用于创建关键字索引，执行的方法为 self._process_keyword_index
                    self._process_keyword_index 中先获取了dataset 对象，然后创建一个 Keyword 实例。
                            Keyword.create 使用 Jieba分词 进行关键词的提取，并且将关键词保存到 document_segment 对象中
                                    最后将关键词保存到数据库中，或者文件系统中
                    将之前标记的 indexing 状态的 DocumentSegment 的状态更新为 completed
            创建一个线程池，执行 self._process_chunk 方法
                    self._process_chunk 先预估大致会消耗的 token 的数量
                    调用 index_processor.load，底层调用 ParagraphIndexProcessor.load
                            load 先是创建 Vector 实例，然后调用 create 方法创建 embeddings
                                    第一步：调用 embed_documents，会根据每个 document.page_content  的hash值，使用的模型，provider 的名称来判断该段文件之前是否向量化过
                                            如果已经向量化过了，那么直接从数据库中查询结果即可。否则将文档下标标记加入一个 embedding_queue_indices 列表中
                                            如果没有向量化，那么获取到当前模型的 max_chunks，按照 max_chunks 拆分这个文本，得到每次向量化的 chunk 内容
                                            然后调用 self._model_instance.invoke_text_embedding 方法进行对应模型的，向量化接口的调用。会调用 invoke 方法，然后调用 self._invoke。self._invoke 对应的就是每个模型实例的向量化接口的具体实现
                                            得到向量化的结果后，会和文本的 hash 值进行绑定，最终存储在 Embedding 表中。会记录 模型名称，provider，hash值，pickle 序列化后的 embedding 值。
                                            如此往复，直到所有文本都向量化结束，最后返回所有向量列表，向量值的顺序就是文本的顺序
                                    第二步：得到向量结果后（可能是新生成的，也可能是直接从数据库获取到以前的）需要把向量保存到向量数据库中
                                            调用 self._vector_processor.create 方法，create 是一个抽象方法，下面有很多的向量数据库的具体实现
                                            这里有个疑问？假如文本相同，那么向量数据库中会存储相同的数据，不会进行去重判断？
                                            如果一段文本进行不同模型的向量化，那的确是可能的。那如何防止同一段文本，同一种模型，保存重复的结果呢?
                    将 DocumentSegment 状态从 indexing 更新为 completed
            更新 document 的 after_indexing_status 状态为 completed


duplicate_document_indexing_task
    1、根据传入的 dataset_id 获取 dataset 对象
    2、判断一些限制
    3、遍历每一个 document_id, 获取到每一个 document_id 的 DocumentSegment 对象
    4、获取所有 DocumentSegment 对象的 index_node_id，得到一个 index_node_ids 列表
    5、调用 index_processor.clean 方法，删除这些 index_node_id
            clean 方法一个接口，具体的实现中，如果索引技术是 high_quality，则需要删除向量数据库中的数据。
                    vector.delete_by_ids(node_ids) 方法中，node_ids 就是传入的 index_node_id。会去向量数据库中删除这些数据
                    在向量数据的插入方法 add_texts 中，也会从 document 对象的 metadata 中提取出 doc_id 进行插入。所以删除的时候就可以根据这些 id 进行查询删除了。
            如果需要删除 keywords，则也会将 keyword 进行删除
    6、然后再在数据库中删除这些 segment
    7、处理完旧数据后，和 document_indexing_task 一样，会创建 IndexingRunner 对象，调用 indexing_runner.run(documents) 方法，传入  documents 对象列表
    8、接下来就和 document_indexing_task 任务中的步骤5接下来的一致了。
    9、总结下来就是多了一个删除旧数据的过程（删除向量数据库中的数据和后端缓存中的数据，其中后端的删除中，并没有删除Embedding表中的数据，只是删除了 DocumentSegment 记录）

    index_node_id 是什么时候绑定到 DocumentSegment 中的，又是怎么和向量数据库进行绑定的？因为删除的时候这个ID显得尤为重要？需要根据这个 ID 去向量数据库中进行数据的删除！
        DocumentSegment 对象 index_node_id 的绑定，是在 document_indexing_task 过程中的，self._load_segments -> add_documents 方法下进行绑定的
        这里在创建 DocumentSegment 对象的时候，将 index_node_id 的值，设置为了 当前文档元数据中的doc_id，即 doc.metadata['doc_id']。


document_indexing_update_task
    1、传入参数为 dataset_id，document_id。首先获取到 document 对象。
    2、将 document indexing_status 状态标记为 parsing
    3、查询 document 对应的 segments 列表，获取到所有的 index_node_id
    4、调用  index_processor.clean(dataset, index_node_ids) 在向量数据库中删除这些数据
    5、再在数据库中删除 segments 数据
    6、最后重新实例化 IndexingRunner 对象，调用 indexing_runner.run([document]) 方法，重新将 document 向量化


对比 Langchain 的实现：
    https://python.langchain.com/v0.1/docs/modules/data_connection/indexing/
    LangChain 利用记录管理器（RecordManager）来管理索引
    在索引内容时，会为每个文档计算哈希值，并将以下信息存储在记录管理器中：
        1、文档哈希（页面内容和元数据的哈希）
        2、时间
        3、source id
    Langchain 提供了三种模式供选择，分别是 None，Incremental，Full
        None 模式：此模式不会自动清理旧版本的内容；但是，它仍会进行内容重复数据删除。举例：当我进行三份相同文本进行索引时，最终向量数据库中只会有一份。且以前的数据不影响。
        incremental 模式：对相同文本再次索引时将会被跳过。但是如果对已经存在的索引进行更新，那么则会写入新版本，并删除所有共享同一 source 的旧版本
        full 模式：任何未传递到索引函数且存在于向量存储中的文档都将被删除。也就是说只会保存当前我提交的，之前的都会被删除。


运行向量化
    修改 .env 配置文件中的 CELERY_BROKER_URL 为 CELERY_BROKER_URL=redis://:difyai123456@localhost:6379/2
    celery -A app.celery worker -P gevent -c 1 --loglevel INFO -Q dataset,generation,mail
    运行后通过接口 /console/api/datasets/bf2d67b6-74bc-498b-8e0e-0c03431461de/batch/20240609135937173902/indexing-status 查询索引状态
    通过 dataset_id, batch id 获取到 document 列表。然后查询 document 下每个 DocumentSegment 完成状态的个数和总的个数。最后得到一个 documents_status 列表。


召回测试
    接口 /console/api/datasets/bf2d67b6-74bc-498b-8e0e-0c03431461de/hit-testing
    发起请求时，会传入检索的方式（全文检索，向量检索，混合检索），如果有 reranking_model 模型，还会传入相关信息。然后就是 score_threshold，top_k 等参数

    过程为：
    1。获取 dataset，并且校验 dataset 的权限，接口参数
    2. 调用 HitTestingService.retrieve 函数，主要逻辑都是在其中完成的
    3. 在函数中 首选获取 retrieval_model，如果没有则使用默认的
    4. 根据 dataset 的 embedding_model_provider 和 embedding_model 获取对应的 embedding_model
    5. embeddings = CacheEmbedding(embedding_model) 得到一个可缓存的 embedding 对象
    6. 调用 RetrievalService.retrieve 方法进行检索，传入 retrival_method检索方法，dataset_id，测试的文本query，top_k，score_threshold，reranking_model 等参数
    7. retrieve 中，首先进行一些简单的校验，校验 dataset 的 document 数量，segment 数量等，如果为0，则就不必继续了
    8. retrival_method 有几种值：keyword_search（关键字搜索），semantic_search（语义搜索），full_text_search（全文搜索），hybrid_search（混合搜索）
        如果 retrival_method 为 keyword_search，则启动一个线程，在线程中执行 RetrievalService.keyword_search 方法
            keyword_search 中，根据 dataset 获取一个 Keyword 实例，然后执行 keyword.search 搜索，得到对应的 documents
        如果 retrival_method 为 semantic_search 或 hybrid_search，则启动一个线程，在线程中执行 RetrievalService.embedding_search 方法
            embedding_search 中，根据 dataset 获取一个 Vector 实例，然后执行 vector.search_by_vector 方法
                 首先对 传入的 query 进行 embedding，这其中使用 redis 进行了缓存，如果 hash 的 key 在 redis 中存在则直接取出，否则调用对应的模型进行 embedding，然后在缓存在 redis 中。这一步的结果的是得到一个 query_vector
                 调用具体的向量数库的 search_by_vector方法，将上一步得到的 query_vector 传入。然后执行查询，查询得到满足条件的 text，将 text 封装成一个 Document 对象，最后返回一个 Document 列表。
            如果选择了 reranking_model，那么还会实例话一个数据后处理的 DataPostProcessor，然后执行 data_post_processor.invoke 方法，参数就是上面从向量数据库中查出来的数据，query，score_threshold，top_n
                DataPostProcessor 中有两种对数据处理的方法，一种是 rerank，另一种是 reorder。rerank 是需要模型的，reorder 不需要。
                invoke 中会尝试执行这两种方法，先执行rerank，然后再执行 reorder（不过没在代码中看到会让 reorder 执行的地方，reorder_enabled 为 True 才会执行）
                DataPostProcessor 在实例话的时候，会根据名次和租户ID，获取得到对应的 rerank 模型实例。
                rerank_runner.run 方法的执行，调用对应 rerank 模型的 invoke_rerank 方法，底层会去调用每个 rerank 模型的接口。最后将得到的 rerank_documents 返回
                如果使用了reorder，那么将 rerank_documents 传入 reorder_runner 中继续执行。reorder_runner 中 run 方法，将奇数下标的元素保持原有顺序,而偶数下标的元素则被反转。例如 documents = [doc1, doc2, doc3, doc4, doc5, doc6, doc7, doc8]，最后结果为  new_documents = [doc1, doc8, doc3, doc6, doc5, doc4, doc7, doc2]
                最后将处理好的 document 列表返回
        如果 retrival_method 为 full_text_search 或 hybrid_search，则启动一个线程，在线程中执行 RetrievalService.full_text_index_search 方法
            full_text_index_search 中，根据 dataset 获取一个 Vector 实例，然后执行 vector.search_by_full_text 方法得到对应的 documents
            如果有 reranking_model 模型，则同样执行 DataPostProcessor invoke 方法，进行数据后处理
    9. 如果 retrival_method 为 hybrid_search，则还会进行一次 DataPostProcessor.invoke，只不过这里的 top_k 是传入的 top_k，而之前的 top_k 是 document 的长度。
        这里以 https://jina.ai/reranker 的 rerank 模型为例。可以看到 rerank api 的参数。
    10. 将 all_documents 做为 retrieve 的返回结果，接下来传入到 compact_retrieve_response 当中
    11. 在 compact_retrieve_response 中，循环 documents，拿去到每一个 document 的 doc_id，然后去查询 DocumentSegment，最后得到的 DocumentSegment 就是从数据库中查询出来的相似的文本片段。最后将结果返回前端。



聊天助手
    接口 console/api/apps/2129de6c-ccff-451e-819e-3e6d665b5e68/chat-messages
    调用时，会将填写的参数 input，query 问题，response_mode，model_config 等参数传入

    调用 AppGenerateService.generate 方法，方法传入 app_model，user，streaming 等关键参数。方法中对 app_model.mode 进行区分，执行不同的方法
    这里我们的应用类型是聊天助手，所以 app_model.mode 是 CHAT，所以回调用 ChatAppGenerator().generate 方法
    ChatAppGenerator 继承自 MessageBasedAppGenerator，MessageBasedAppGenerator 继承自 BaseAppGenerator

    1. ChatAppGenerator().generate 中，先获取 conversation
    2. 然后根据 app_model 和 conversation 获取 app model config, 如果是 debug 模式，则允许用户传递的 model config 覆盖 据 app_model 和 conversation 查询出来的
    3. 解析文件得到 file_objs
    4. 转换为 app config
    5. 初始化 application generate entity
    6. 初始化 generate records
    7. 初始化 queue manager
    8. 启动线程，调用  _generate_worker 方法
        8.1 线程中第一步就是根据 conversation_id 和 message_id 获取 conversation 和 message
        8.2 实例话一个 ChatAppRunner 对象，并调用 run 方法，传入 application_generate_entity，queue_manager，conversation 和 message
            8.2.1 中先解析出用户的输入自定义变量 inputs，用户的问题 query 和 文件 files
            8.2.2 判断 token 数量是否足够
                加载模型实例，获取模型的最大 token 数
                将用户自定义的变量和问题组合成为一个 prompt，然后计算 prompt 的需要的 token 数
                最后计算 token 是否足够，不足够则返回异常，够则返回剩余的 token 数量
            8.2.3 组合输入和模板，得到提示信息
            8.2.4 进行敏感词检测，如果抛出了ModerationException异常，则直接调用 self.direct_output 然后 return，不再继续
				敏感词检测是一个 ModerationFactory 工厂函数，调用 moderation_for_inputs 方法，目前支持三种方式，1.API，2.关键词，3.openai
            8.2.5 annotation reply
				标注回复是可以添加回复，由一个问题和一个答案组成，当用户的问题和标注回复中的问题一致时，那么就返回指定的内容。
				查询到当前app的AppAnnotationSetting后，获取相关模型的 provider_name，model_name，score_threshold等，然后去向量数据库中查询
				查询的时候，是使用用户提出的问题去查询，如果项目相似度分数 > score_threshold
					则取出这个 Document 的 annotation_id，然后到这个 annotation，然后将预设的回复进行返回
				上述返回后，就往队列里面塞一条消息，就不继续往后执行了

				添加标注回复时，调用 /console/api/apps/xxx/annotations 接口，会往 MessageAnnotation 表中添加记录。
				并调用 add_annotation_to_index_task 异步方法，异步方法中创建的 Document 的内容为标记回复的问题，元数据中关联了 annotation_id
				document = Document(
					page_content=question,
					metadata={
						"annotation_id": annotation_id,
						"app_id": app_id,
						"doc_id": annotation_id
					}
				)
				
            8.2.6 填写来自外部数据工具的变量输入，获取下上文信息
				在调用上下文的 DatasetRetrieval.retrieve 时，判断了模型是否有 TOOL_CALL 或 MULTI_TOOL_CALL 的能力。
				如果有，则 planning_strategy 为 ROUTER 模式，否则为 REACT_ROUTER 模式 
				然后在根据用户选择的是N选1召回，还是多路召回，调用不同的方法，分别是 single_retrieve 方法和 multiple_retrieve 方法
				single_retrieve：
					当 planning_strategy 为 ROUTER 时：则利用大模型的 FunctionCall 能力，调用 FunctionCallMultiDatasetRouter 方法。
					如果传入的有效的dataset(界面中选择的知识库)的个数等于0时直接返回None,个数为1则返回dataset的名称,其余情况则使用FunctionCall
				multiple_retrieve:
					根据有效的 dataset 的个数，使用相同数量的线程，去线程中查询，得到相关的 documents
					然后在使用 rerank 模型对相关的 documents 进行排序
            8.2.7 再次组合输入和模板，得到提示信息
            8.2.9 check hosting moderation
            8.2.10 加载工具变量
            8.2.11 初始化模型实例
            8.2.12 根据 LLM 模型 调整 function call 策略，得到不同的 AgentRunner
            8.2.13 实例化 AgentRunner，调用 run 方法。得到结果
            8.2.14 处理返回结果

	9. 得到 response
		9.1 调用  generate_task_pipeline.process() 方法
			9.1.1 判断类型是否为 COMPLETION，如果是 COMPLETION 则需要为当前对话生成一个名称（新启一个线程实现）
			9.1.2 继续往下执行 generator = self._process_stream_response() 方法
				在 _process_stream_response 方法中，使用 queue_manager.listen() 方法监听推送到里面的消息。
				获取到消息后，根据消息不同的 event 类型进行不同的处理，返回不同的对象，目前 event 类型有：
					QueueStopEvent，QueueMessageEndEvent，QueueRetrieverResourcesEvent，QueueAnnotationReplyEvent，
					QueueAgentThoughtEvent，QueueMessageFileEvent，QueueLLMChunkEvent，QueueAgentMessageEvent，
					QueueLLMChunkEvent，QueueMessageReplaceEvent，QueuePingEvent
					
	10. 最后 AgentChatAppGenerateResponseConverter.convert 将上述的 response 转换后返回出去






文本生成应用
	调用 /console/api/apps/xxx/completion-messages 接口
	调用  AppGenerateService.generate 方法，app_model.mode 是 'completion'
	
		
	1. CompletionAppGenerator().generate 中和聊天助手不一样，这里就没有 conversation，所以 conversation 为 None
	2. 然后根据 app_model 和 conversation 获取 app model config, 如果是 debug 模式，则允许用户传递的 model config 覆盖 app model config
	3. 解析文件得到 file_objs
    4. 转换为 app config
    5. 初始化 application generate entity
    6. 初始化 generate records
		 得到一个 conversation 和 message 对象
    7. 初始化 queue manager
	8. 启动线程，调用  _generate_worker 方法
		8.1 线程中第一步就是根据 message_id 获取 message
		8.2 实例化一个  CompletionAppRunner() 对象，并且调用 run 方法
			8.2.1 中先解析出用户的输入自定义变量 inputs，用户的问题 query 和 文件 files
            8.2.2 判断 token 数量是否足够
                加载模型实例，获取模型的最大 token 数
                将用户自定义的变量和问题组合成为一个 prompt，然后计算 prompt 的需要的 token 数
                最后计算 token 是否足够，不足够则返回异常，够则返回剩余的 token 数量
			8.2.3 组织 prompt message
			8.2.4 敏感词检测
			8.2.5 填写来自外部数据工具的变量输入
			8.2.6 获取上下文信息
			8.2.7 再次重新组织 prompt message
			8.2.8 check hosting moderation
			8.2.9 如果sum(prompt_token + max_token)超过模型令牌限制，则重新计算最大令牌
			8.2.10 初始化模型实例，得到 model_instance
			8.2.11 执行 model_instance.invoke_llm 方法，得到 invoke_result
				8.2.11.1 执行  LargeLanguageModel 的 invoke 方法，这就和之前的一样的了
			8.2.12 处理 invoke_result
			
	9. 得到 response
	10. 最后 CompletionAppGenerateResponseConverter.convert 将上述的 response 转换后返回出去
	
	
	
Agent
	调用 /console/api/apps/xxx/chat-messages接口
	调用  AgentChatAppGenerator.generate 方法，app_model.mode 是 'agent-chat'
	
	1. 如果参数中有 conversation_id 则先获取 conversation
	2. 根据 conversation 和 app_model 获取 app model config
	3. 如果是 debug 模式，则允许用户传递的 model config 覆盖  model config 
	4. 解析文件得到 file_objs 
	5. 转换为 app config
	6. 初始化 application generate entity
	7. 初始化 generate records
		 得到一个 conversation 和 message 对象
	8. 初始化 queue manager
	9. 启动线程，调用  _generate_worker 方法
		9.1 线程中第一步就是根据 message_id 获取 message 和 根据 conversation_id 获取 conversation
		9.2 实例化一个  AgentChatAppRunner() 对象，并且调用 run 方法
			9.2.1 先解析出用户的输入自定义变量 inputs，用户的问题 query 和 文件 files
			9.2.2 判断 token 数量是否足够
			9.2.3 组织 prompt message
			9.2.4 敏感词检测
			9.2.5 annotation reply 标注回复
			9.2.6 填写来自外部数据工具的变量输入(如果存在)
			9.2.7 组织 prompt message
			9.2.8 check hosting moderation
			9.2.8 加载工具变量，并将db变量转换为工具变量
			9.2.9 初始化 model 实例
			9.2.10 组织 prompt message
			9.2.11 根据LLM模型确认使用  function call 模式还是 CHAIN_OF_THOUGHT（链式思考模式）
				如果是 CHAIN_OF_THOUGHT（链式思考）模式，且LLM模式是 CHAT模式，则使用 CotChatAgentRunner
				如果是 CHAIN_OF_THOUGHT（链式思考）模式，且LLM模式是 COMPLETION模式，则使用 CotCompletionAgentRunner

				如果是 FUNCTION_CALLING 模式，则使用 FunctionCallAgentRunner
				
			9.2.12 实例化上述 AgentRunner，并且调用 run 方法，得到 invoke_result
				- 如果是 FunctionCallAgentRunner 的 run方法
					第一步就是调用  self._init_prompt_tools 方法，得到  tool_instances, prompt_messages_tools 
					获取调用的最大步数 max_iteration_steps
					while 循环运行每一步，直到步数达到 max_iteration_steps，或者 function_call_state 为 False
						循环第一步会组织 prompt 和 计算 max tokens
						循环中会先进行大模型的调用，model_instance.invoke_llm 得到 chunks
						根据大模型是否支持 stream_tool_call 进行不同的处理
						循环chunks，判断每个 chunk 是否有工具调用（大模型选择出工具）
						如果没有选择出工具，则大模型返回出什么，就 yield 出什么
						如果选择出了工具，那么就在下面循环调用每个工具。然后再次进行循环。
						再次循环时候，prompt 就会更新，带上选出的工具信息，然后再次调用大模型。
				- 如果是 CotChatAgentRunner 或 CotCompletionAgentRunner 的 run方法 
					大致流程和上述一致，只是没有了 tools，而是判断是否有 action。如果没有 action，直接返回最终答案
					否则则判断 action 是否为 final answer。如果不是，则去调用每一个 action。
					然后重复上述步骤，直到步数达到 max_iteration_steps，或者 function_call_state 为 False
					
			9.2.13 调用 self._handle_invoke_result 处理上述 invoke_result
	10.得到 response
	11. 最后 AgentChatAppGenerateResponseConverter.convert 将上述的 response 转换后返回出去
	
	
	
	
	
	
	
	
	
	
	
	
	
	


	
问题：
	1、如果 prompt 的token过长，怎么处理？
	
	2、如果 prompt 的 token 不过长，但是加上上下文之后过长怎么处理？

	3、最多可以携带几轮会话历史？

	4、agent 应用中，如果模型不支持 function call 时怎么办？

	5、用户输入和模型输出出现敏感词怎么处理？

	6、怎么添加和拓展工具？

	7、在 Agent 中，如果匹配不到对应的工具类怎么处理？

	
	
	
	

	
	
	
	
	
	
	
	
	
	
	
	
	
	
	
	
	
	
	
	
	
	
	
	
	
	
	
	
	
	
	
	
	
	
	
	
	
	
	
	
	
	
	
	


















    








