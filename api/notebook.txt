/console/api/datasets/init
    1、如果索引方式是 high_quality ，则判断是否有 Embedding 模型，然后调用 save_document_without_dataset_id 方法
    2、如果索引方式是 high_quality，则需要为 dataset 创建一个 dataset_collection_binding，获取到 dataset_collection_binding_id
         如果索引方式不是 high_quality，则 dataset_collection_binding_id 为 None
    3、同理，如果索引方式是 high_quality，则需要获取到 embedding_model 和 retrieval_model
         如果索引方式不是 high_quality，则 embedding_model，retrieval_model 为 None
    4、创建 DataSet，需要用到上述的 retrieval_model，collection_binding_id，embedding_model 等参数
    5、调用 save_document_with_dataset_id 方法
    6、如果 dataset 为空，则更新dataset 的 data_source_type，indexing_technique
    7、绑定当前 dataset 的 ProcessRule，便于后续用到
    8、如果当前文件是上传的，那么循环上传的文件列表创建或更新 document
            根据文件名称，dataset id，租户名称等进行查询，如果有则更新 document，没有则新建 document
            将之前就有的 document 的 id 放入 duplicate_document_ids 中，新建的 document id 放入 document_ids 中
    9、将新建的 document 列表传入  document_indexing_task 异步任务中
    10、将更新的 document 列表传入 duplicate_document_indexing_task 异步任务中


document_indexing_task
    1、根据传入的 dataset_id 获取 dataset 对象
    2、判断一些限制
    3、根据传入的 document_ids，循环更新 document 对象，更新 indexing_status 为 parsing，更新 processing_started_at 时间
    4、初始化 IndexingRunner，执行 run 方法，传入 documents 对象列表
    5、查询 DatasetProcessRule 的 processing_rule。得到 处理规则
    6、根据 index_type 获取到 index_processor 对象。
    7、调用 self._extract 提取文本，返回值为 list[Document]
            self._extract 底层调用 index_processor.extract，最终调用 ExtractProcessor.extract
            更新 after_indexing_status 值为 splitting
    8、将提取出的 list[Document] 传入 self._transform
            self._transform 底层调用 index_processor.transform
    9、transform 中首先根据 process_rule 获取到一个 splitter
    10、循环传入的 documents 列表（步骤7提取的结果）
            在循环中为每个 document 对象添加 doc_id 和 doc_hash 的元数据。doc_id 是 uuid，doc_hash 则是根据内容计算的
            在循环中，第一步调用 CleanProcessor.clean 清除额外信息（空格，邮箱，用户可在前端选择）
            第二步调用 splitter.split_documents([document])，创建 Document 对象。这里叫做 document_nodes
            第三步为每个 Document 对象赋予一个 doc_id 和 doc_hash 值。并且去除文字中的句号
    11、返回 all_documents ，里面是所有处理好的 Document，是一个列表
    12、调用  self._load_segments(dataset, dataset_document, documents)，documents 是 11 步的返回结果
            创建一个 DatasetDocumentStore 实例
            调用 add_documents 方法，将 documents 列表中的每条数据，采用 create or update 的策略保存为一个 DocumentSegment 对象
    13、更新 document 的 after_indexing_status 值为 indexing，更新 segment 的状态为 indexing
    14、调用 self._load
            新启一个线程，用于创建关键字索引，执行的方法为 self._process_keyword_index
                    self._process_keyword_index 中先获取了dataset 对象，然后创建一个 Keyword 实例。
                            Keyword.create 使用 Jieba分词 进行关键词的提取，并且将关键词保存到 document_segment 对象中
                                    最后将关键词保存到数据库中，或者文件系统中
                    将之前标记的 indexing 状态的 DocumentSegment 的状态更新为 completed
            创建一个线程池，执行 self._process_chunk 方法
                    self._process_chunk 先预估大致会消耗的 token 的数量
                    调用 index_processor.load，底层调用 ParagraphIndexProcessor.load
                            load 先是创建 Vector 实例，然后调用 create 方法创建 embeddings
                                    第一步：调用 embed_documents，会根据每个 document.page_content  的hash值，使用的模型，provider 的名称来判断该段文件之前是否向量化过
                                            如果已经向量化过了，那么直接从数据库中查询结果即可。否则将文档下标标记加入一个 embedding_queue_indices 列表中
                                            如果没有向量化，那么获取到当前模型的 max_chunks，按照 max_chunks 拆分这个文本，得到每次向量化的 chunk 内容
                                            然后调用 self._model_instance.invoke_text_embedding 方法进行对应模型的，向量化接口的调用。会调用 invoke 方法，然后调用 self._invoke。self._invoke 对应的就是每个模型实例的向量化接口的具体实现
                                            得到向量化的结果后，会和文本的 hash 值进行绑定，最终存储在 Embedding 表中。会记录 模型名称，provider，hash值，pickle 序列化后的 embedding 值。
                                            如此往复，直到所有文本都向量化结束，最后返回所有向量列表，向量值的顺序就是文本的顺序
                                    第二步：得到向量结果后（可能是新生成的，也可能是直接从数据库获取到以前的）需要把向量保存到向量数据库中
                                            调用 self._vector_processor.create 方法，create 是一个抽象方法，下面有很多的向量数据库的具体实现
                                            这里有个疑问？假如文本相同，那么向量数据库中会存储相同的数据，不会进行去重判断？
                                            如果一段文本进行不同模型的向量化，那的确是可能的。那如何防止同一段文本，同一种模型，保存重复的结果呢?
                    将 DocumentSegment 状态从 indexing 更新为 completed
            更新 document 的 after_indexing_status 状态为 completed


duplicate_document_indexing_task
    1、根据传入的 dataset_id 获取 dataset 对象
    2、判断一些限制
    3、遍历每一个 document_id, 获取到每一个 document_id 的 DocumentSegment 对象
    4、获取所有 DocumentSegment 对象的 index_node_id，得到一个 index_node_ids 列表
    5、调用 index_processor.clean 方法，删除这些 index_node_id
            clean 方法一个接口，具体的实现中，如果索引技术是 high_quality，则需要删除向量数据库中的数据。
                    vector.delete_by_ids(node_ids) 方法中，node_ids 就是传入的 index_node_id。会去向量数据库中删除这些数据
                    在向量数据的插入方法 add_texts 中，也会从 document 对象的 metadata 中提取出 doc_id 进行插入。所以删除的时候就可以根据这些 id 进行查询删除了。
            如果需要删除 keywords，则也会将 keyword 进行删除
    6、然后再在数据库中删除这些 segment
    7、处理完旧数据后，和 document_indexing_task 一样，会创建 IndexingRunner 对象，调用 indexing_runner.run(documents) 方法，传入  documents 对象列表
    8、接下来就和 document_indexing_task 任务中的步骤5接下来的一致了。
    9、总结下来就是多了一个删除旧数据的过程（删除向量数据库中的数据和后端缓存中的数据，其中后端的删除中，并没有删除Embedding表中的数据，只是删除了 DocumentSegment 记录）

    index_node_id 是什么时候绑定到 DocumentSegment 中的，又是怎么和向量数据库进行绑定的？因为删除的时候这个ID显得尤为重要？需要根据这个 ID 去向量数据库中进行数据的删除！
        DocumentSegment 对象 index_node_id 的绑定，是在 document_indexing_task 过程中的，self._load_segments -> add_documents 方法下进行绑定的
        这里在创建 DocumentSegment 对象的时候，将 index_node_id 的值，设置为了 当前文档元数据中的doc_id，即 doc.metadata['doc_id']。


document_indexing_update_task
    1、传入参数为 dataset_id，document_id。首先获取到 document 对象。
    2、将 document indexing_status 状态标记为 parsing
    3、查询 document 对应的 segments 列表，获取到所有的 index_node_id
    4、调用  index_processor.clean(dataset, index_node_ids) 在向量数据库中删除这些数据
    5、再在数据库中删除 segments 数据
    6、最后重新实例化 IndexingRunner 对象，调用 indexing_runner.run([document]) 方法，重新将 document 向量化


对比 Langchain 的实现：
    https://python.langchain.com/v0.1/docs/modules/data_connection/indexing/
    LangChain 利用记录管理器（RecordManager）来管理索引
    在索引内容时，会为每个文档计算哈希值，并将以下信息存储在记录管理器中：
        1、文档哈希（页面内容和元数据的哈希）
        2、时间
        3、source id
    Langchain 提供了三种模式供选择，分别是 None，Incremental，Full
        None 模式：此模式不会自动清理旧版本的内容；但是，它仍会进行内容重复数据删除。举例：当我进行三份相同文本进行索引时，最终向量数据库中只会有一份。且以前的数据不影响。
        incremental 模式：对相同文本再次索引时将会被跳过。但是如果对已经存在的索引进行更新，那么则会写入新版本，并删除所有共享同一 source 的旧版本
        full 模式：任何未传递到索引函数且存在于向量存储中的文档都将被删除。也就是说只会保存当前我提交的，之前的都会被删除。


运行向量化
    修改 .env 配置文件中的 CELERY_BROKER_URL 为 CELERY_BROKER_URL=redis://:difyai123456@localhost:6379/2
    celery -A app.celery worker -P gevent -c 1 --loglevel INFO -Q dataset,generation,mail
    运行后通过接口 /console/api/datasets/bf2d67b6-74bc-498b-8e0e-0c03431461de/batch/20240609135937173902/indexing-status 查询索引状态
    通过 dataset_id, batch id 获取到 document 列表。然后查询 document 下每个 DocumentSegment 完成状态的个数和总的个数。最后得到一个 documents_status 列表。


召回测试
    接口 /console/api/datasets/bf2d67b6-74bc-498b-8e0e-0c03431461de/hit-testing
    发起请求时，会传入检索的方式（全文检索，向量检索，混合检索），如果有 reranking_model 模型，还会传入相关信息。然后就是 score_threshold，top_k 等参数

    过程为：
    1。获取 dataset，并且校验 dataset 的权限，接口参数
    2. 调用 HitTestingService.retrieve 函数，主要逻辑都是在其中完成的
    3. 在函数中 首选获取 retrieval_model，如果没有则使用默认的
    4. 根据 dataset 的 embedding_model_provider 和 embedding_model 获取对应的 embedding_model
    5. embeddings = CacheEmbedding(embedding_model) 得到一个可缓存的 embedding 对象
    6. 调用 RetrievalService.retrieve 方法进行检索，传入 retrival_method检索方法，dataset_id，测试的文本query，top_k，score_threshold，reranking_model 等参数
    7. retrieve 中，首先进行一些简单的校验，校验 dataset 的 document 数量，segment 数量等，如果为0，则就不必继续了
    8. retrival_method 有几种值：keyword_search（关键字搜索），semantic_search（语义搜索），full_text_search（全文搜索），hybrid_search（混合搜索）
        如果 retrival_method 为 keyword_search，则启动一个线程，在线程中执行 RetrievalService.keyword_search 方法
            keyword_search 中，根据 dataset 获取一个 Keyword 实例，然后执行 keyword.search 搜索，得到对应的 documents
        如果 retrival_method 为 semantic_search 或 hybrid_search，则启动一个线程，在线程中执行 RetrievalService.embedding_search 方法
            embedding_search 中，根据 dataset 获取一个 Vector 实例，然后执行 vector.search_by_vector 方法
                 首先对 传入的 query 进行 embedding，这其中使用 redis 进行了缓存，如果 hash 的 key 在 redis 中存在则直接取出，否则调用对应的模型进行 embedding，然后在缓存在 redis 中。这一步的结果的是得到一个 query_vector
                 调用具体的向量数库的 search_by_vector方法，将上一步得到的 query_vector 传入。然后执行查询，查询得到满足条件的 text，将 text 封装成一个 Document 对象，最后返回一个 Document 列表。
            如果选择了 reranking_model，那么还会实例话一个数据后处理的 DataPostProcessor，然后执行 data_post_processor.invoke 方法，参数就是上面从向量数据库中查出来的数据，query，score_threshold，top_n
                DataPostProcessor 中有两种对数据处理的方法，一种是 rerank，另一种是 reorder。rerank 是需要模型的，reorder 不需要。
                invoke 中会尝试执行这两种方法，先执行rerank，然后再执行 reorder（不过没在代码中看到会让 reorder 执行的地方，reorder_enabled 为 True 才会执行）
                DataPostProcessor 在实例话的时候，会根据名次和租户ID，获取得到对应的 rerank 模型实例。
                rerank_runner.run 方法的执行，调用对应 rerank 模型的 invoke_rerank 方法，底层会去调用每个 rerank 模型的接口。最后将得到的 rerank_documents 返回
                如果使用了reorder，那么将 rerank_documents 传入 reorder_runner 中继续执行。reorder_runner 中 run 方法，将奇数下标的元素保持原有顺序,而偶数下标的元素则被反转。例如 documents = [doc1, doc2, doc3, doc4, doc5, doc6, doc7, doc8]，最后结果为  new_documents = [doc1, doc8, doc3, doc6, doc5, doc4, doc7, doc2]
                最后将处理好的 document 列表返回
        如果 retrival_method 为 full_text_search 或 hybrid_search，则启动一个线程，在线程中执行 RetrievalService.full_text_index_search 方法
            full_text_index_search 中，根据 dataset 获取一个 Vector 实例，然后执行 vector.search_by_full_text 方法得到对应的 documents
            如果有 reranking_model 模型，则同样执行 DataPostProcessor invoke 方法，进行数据后处理
    9. 如果 retrival_method 为 hybrid_search，则还会进行一次 DataPostProcessor.invoke，只不过这里的 top_k 是传入的 top_k，而之前的 top_k 是 document 的长度。
        这里以 https://jina.ai/reranker 的 rerank 模型为例。可以看到 rerank api 的参数。
    10. 将 all_documents 做为 retrieve 的返回结果，接下来传入到 compact_retrieve_response 当中
    11. 在 compact_retrieve_response 中，循环 documents，拿去到每一个 document 的 doc_id，然后去查询 DocumentSegment，最后得到的 DocumentSegment 就是从数据库中查询出来的相似的文本片段。最后将结果返回前端。







    








